{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing and PCA data reduction of input data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import sklearn.decomposition as pca\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load in datasets and perform pandas cleaning up before scaling</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_data = pd.read_csv('./pretrain_features.csv.zip')\n",
    "pretrain_start = pd.DataFrame(pretrain_data,columns=['Id','smiles'])\n",
    "pretrain_trimmed = pretrain_data.drop(['Id','smiles'], axis=1)\n",
    "\n",
    "train_data = pd.read_csv('./train_features.csv.zip')\n",
    "train_start = pd.DataFrame(train_data,columns=['Id','smiles'])\n",
    "train_trimmed = train_data.drop(['Id','smiles'], axis=1)\n",
    "\n",
    "test_data = pd.read_csv('./test_features.csv.zip')\n",
    "test_start = pd.DataFrame(test_data,columns=['Id','smiles'])\n",
    "test_trimmed = test_data.drop(['Id','smiles'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Preprocess data to achieve mean = 0 and var = 1 NOTE scaler trained on pretrain features but applied to all three</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(pretrain_trimmed)\n",
    "pretrain_data_processed = scaler.transform(pretrain_trimmed)\n",
    "train_data_processed = scaler.transform(train_trimmed)\n",
    "test_data_processed = scaler.transform(test_trimmed)\n",
    "# print(f\"mean of scaler: {data_processed.mean(axis=0)}\")\n",
    "# print(f\"size of scaler: {data_processed.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Apply dimensionality reduction to break the 1000 features down into principle components NOTE not sure if same pca is good procedure but I think the more similar the NN input the easier to apply transfer learning</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of pca_datasets: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "pca_handler = pca.PCA(n_components=10)  # TODO see with how many features per molecule we perform best \n",
    "pca_handler.fit(pretrain_data_processed)\n",
    "pca_pretrain_data = pca_handler.transform(pretrain_data_processed)\n",
    "pca_train_data = pca_handler.transform(train_data_processed)\n",
    "pca_test_data = pca_handler.transform(test_data_processed)\n",
    "print(f\"type of pca_datasets: {type(pca_train_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Save the reduced datasets</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain dataset concatenated:           Id                                             smiles  feature_1  \\\n",
      "0          0  c1occ2c1c1ccc3cscc3c1c1ncc3cc(ccc3c21)-c1cccc2...  -4.629602   \n",
      "1          1  C1C=c2c(cc3ncc4c5[SiH2]C=Cc5oc4c3c2=C1)-c1scc2...   0.959931   \n",
      "2          2  C1C=c2c3cccnc3c3c4c[nH]cc4c4cc(cnc4c3c2=C1)-c1...  -4.421394   \n",
      "3          3  [SiH2]1C=Cc2c1csc2-c1cnc2c(c1)c1ccccc1c1cc3ccc...   1.282459   \n",
      "4          4        c1occ2c1c(cc1[se]c3ccncc3c21)-c1cccc2nsnc12  -3.172590   \n",
      "...      ...                                                ...        ...   \n",
      "49995  49995      C1cc2c3-c4[nH]ccc4-ncc3c3c4c[SiH2]cc4ccc3c2c1  -4.543089   \n",
      "49996  49996  [SiH2]1C=c2c3C=C([SiH2]c3c3c4cscc4c4C=C[SiH2]c...   0.618582   \n",
      "49997  49997              C1C=Cc2csc(C3=Cc4ccc5cc[se]c5c4C3)c12   5.223163   \n",
      "49998  49998  [SiH2]1C=c2c3cc([nH]c3c3c4ccccc4c4ccc5nsnc5c4c...  -3.068306   \n",
      "49999  49999  C1C=c2c3ccccc3c3c(c4ccc(cc4c4=C[SiH2]C=c34)-c3...  -3.606739   \n",
      "\n",
      "       feature_2  feature_3  feature_4  feature_5  feature_6  feature_7  \\\n",
      "0       1.907858  -2.072462  -2.790687  -0.549325  -1.233226  -1.645856   \n",
      "1      -3.398761   1.173293  -0.218691   0.005047   4.780915   1.925038   \n",
      "2       1.593658   1.216766   1.268223   0.535062   0.653172  -1.869499   \n",
      "3       1.658626   0.858395  -3.583970   3.904734   1.587156  -1.034108   \n",
      "4       5.702173   0.015434  -1.513893   1.268228  -1.609655  -0.361176   \n",
      "...          ...        ...        ...        ...        ...        ...   \n",
      "49995  -3.607751   3.824651   5.114275  -0.435119   3.424251   0.496211   \n",
      "49996  -6.023589   2.749241  -1.677369   0.138936   1.620616  -2.012511   \n",
      "49997  -3.786876  -0.501029  -0.489081   2.545525   0.052531   0.586390   \n",
      "49998   3.270210   3.275707  -0.076479   0.866558   4.179176   0.806594   \n",
      "49999  -3.158619   1.602424   5.041774  -3.267748   2.576487  -2.227667   \n",
      "\n",
      "       feature_8  feature_9  feature_10  \n",
      "0      -2.069164  -3.565064   -2.162124  \n",
      "1       2.000319   0.202305    1.320919  \n",
      "2      -3.004621  -4.037786    4.544799  \n",
      "3       0.427778  -4.987563    4.392146  \n",
      "4       0.124882   2.329564   -1.381881  \n",
      "...          ...        ...         ...  \n",
      "49995   0.586416  -0.662113    2.327537  \n",
      "49996   8.361326   0.663985    0.668029  \n",
      "49997  -6.383388   2.835628    1.219386  \n",
      "49998   3.842206   0.719304    0.230346  \n",
      "49999   2.616039  -0.762436    2.693267  \n",
      "\n",
      "[50000 rows x 12 columns]\n",
      "train dataset concatenated:        Id                                             smiles  feature_1  \\\n",
      "0   50000    C1C=c2c3ccoc3c3c4ccccc4c(cc3c2=C1)-c1scc2ccsc12  -0.392920   \n",
      "1   50001  c1cc([se]c1-c1sc(-c2cccc3nsnc23)c2nccnc12)-c1c...   2.203037   \n",
      "2   50002  [SiH2]1C=CC=C1c1cc2cnc3c(sc4ccc5c[nH]cc5c34)c2...  -1.314659   \n",
      "3   50003  C1C=c2ccc3c4cocc4c4c([se]c5cc(-c6cccs6)c6nsnc6...  -4.164995   \n",
      "4   50004  C1c(ccc1-c1sc(-c2nccc3nsnc23)c2ccoc12)-c1scc2c...   5.406500   \n",
      "..    ...                                                ...        ...   \n",
      "95  50095  C1C=c2c(cc3oc4c5CC=Cc5c5c[nH]cc5c4c3c2=C1)-c1c...  -3.504132   \n",
      "96  50096  C1C(=Cc2c1c1c(c3nsnc23)c2[nH]ccc2c2=C[SiH2]C=c...   1.690843   \n",
      "97  50097   c1scc2c1ccc1[nH]c3c4oc(cc4c4nsnc4c3c21)-c1ccccn1  -4.314875   \n",
      "98  50098  [SiH2]1c2cc[nH]c2-c2oc3cc(-c4nccc5nsnc45)c4nsn...   0.120156   \n",
      "99  50099  [SiH2]1cc2c(ccc(-c3cccc4nsnc34)c2c1)-c1ccc(nc1...  -1.167466   \n",
      "\n",
      "    feature_2  feature_3  feature_4  feature_5  feature_6  feature_7  \\\n",
      "0   -2.330069  -3.436942   2.959239  -1.766187  -0.583733   5.718142   \n",
      "1    5.482903   3.052734   1.268971   0.365309   0.164489   0.033824   \n",
      "2   -0.820975   0.877187  -3.660656   0.020809  -1.049812  -4.286345   \n",
      "3    1.108938   1.652130   3.484292   0.542300  -1.605696   0.990113   \n",
      "4    4.902474   0.565337  -0.662431  -5.225921   1.896077   3.904586   \n",
      "..        ...        ...        ...        ...        ...        ...   \n",
      "95  -5.168079   1.366832  -1.355425  -3.700395  -0.930009   0.243471   \n",
      "96  -1.809467   1.946036   0.964024  -0.969757   5.554055   1.998524   \n",
      "97   2.943525  -1.299248  -2.623190  -1.773126   2.640873   0.592792   \n",
      "98   5.043255   3.599758  -2.649039  -1.561875   2.476501   1.933188   \n",
      "99   5.377154   4.489195   2.080520  -3.185127  -3.326964   3.286718   \n",
      "\n",
      "    feature_8  feature_9  feature_10  \n",
      "0   -0.606249  -2.329300   -2.536541  \n",
      "1    0.508039   2.667232   -1.472304  \n",
      "2    4.766176  -0.435772   -3.212569  \n",
      "3   -1.905172   1.262209   -5.432307  \n",
      "4    0.383592   5.019931   -0.789565  \n",
      "..        ...        ...         ...  \n",
      "95  -3.573318   1.670356    0.211927  \n",
      "96   0.877199   2.713447   -0.087497  \n",
      "97   0.118792   0.703229    2.351007  \n",
      "98   1.698271   4.816976   -0.448929  \n",
      "99   3.270552   0.213981    3.580778  \n",
      "\n",
      "[100 rows x 12 columns]\n",
      "test dataset concatenated:          Id                                             smiles  feature_1  \\\n",
      "0     50100  c1cc2c(scc2[nH]1)-c1ccc(cn1)-c1sc(-c2scc3occc2...   7.497489   \n",
      "1     50101   [SiH2]1C=Cc2ncc3c4oc(cc4c4cocc4c3c12)-c1ccc[se]1  -0.655284   \n",
      "2     50102  C1C=c2c3c(oc4cc(C5=CC=CC5)c5nsnc5c34)c3ccccc3c...  -2.305021   \n",
      "3     50103       c1c[se]c(c1)-c1ccc2c(c1)oc1c3[nH]ccc3[se]c21   0.094412   \n",
      "4     50104            c1scc2c1ccc1sc3c([se]c4ccc5nsnc5c34)c21  -2.434153   \n",
      "...     ...                                                ...        ...   \n",
      "9995  60095     C1C=Cc2ccc3c4[nH]c(cc4ncc3c12)-c1scc2cc[nH]c12   2.973315   \n",
      "9996  60096  c1occ2c1ccc1ccc3c([se]c4cc(-c5cccnc5)c5nsnc5c3...  -3.247162   \n",
      "9997  60097      c1ncc(s1)-c1ccc(cn1)-c1ccc(-c2cccnc2)c2nsnc12   1.130029   \n",
      "9998  60098    C1c(ccc1-c1ccccc1)-c1sc(-c2ccc[nH]2)c2[nH]ccc12   4.962671   \n",
      "9999  60099  C1cc2cccc(-c3cnc(s3)-c3sc(-c4cccc5nsnc45)c4cc[...   3.117141   \n",
      "\n",
      "      feature_2  feature_3  feature_4  feature_5  feature_6  feature_7  \\\n",
      "0      1.788701  -5.446082   2.854483  -5.956314   0.519468   5.428475   \n",
      "1     -1.618371  -1.100673  -3.521002   3.861490  -4.160703  -1.273214   \n",
      "2     -0.547694   4.649904  -0.333240  -3.404749  -1.671537   1.469152   \n",
      "3     -0.871413  -3.670665   2.014983   1.374314  -1.886755  -1.956536   \n",
      "4      3.402473  -0.656000  -0.115467   3.486623   1.113510   0.791139   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "9995  -2.180576  -1.345665  -0.311862  -1.507986   6.346145  -0.766708   \n",
      "9996   4.749335  -1.595919  -1.105786   1.637385  -2.664906  -1.125043   \n",
      "9997   6.755537   2.361275   1.155179  -0.448470  -0.893227  -2.515342   \n",
      "9998  -0.883967  -1.171452   1.997670  -7.380224   3.155255  -5.444799   \n",
      "9999   2.240849   8.859899   3.824442  -0.247580  -0.751839  -0.676817   \n",
      "\n",
      "      feature_8  feature_9  feature_10  \n",
      "0      1.703158  -1.672264    5.213258  \n",
      "1      2.213917  -0.375720    1.966959  \n",
      "2     -5.078960   3.860740   -1.605977  \n",
      "3      0.117181   3.565506    2.222684  \n",
      "4     -1.005068   1.429137   -5.632593  \n",
      "...         ...        ...         ...  \n",
      "9995  -2.092564   0.117724    2.097996  \n",
      "9996  -0.388286   0.067799   -0.066482  \n",
      "9997  -1.244810  -3.728338    4.927777  \n",
      "9998   1.951744   2.292615    1.914972  \n",
      "9999   2.199684  -1.919569   -2.073677  \n",
      "\n",
      "[10000 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "feature_cols = ['feature_' + str(i+1) for i in range(pca_train_data.shape[1])] #create label names for the new pca labels\n",
    "#create datasets\n",
    "processed_pretrain_dataset = pd.DataFrame(pca_pretrain_data,columns=feature_cols)\n",
    "# print(f\"pretrain dataset: {processed_pretrain_dataset}\")\n",
    "processed_pretrain_dataset = pd.concat([pretrain_start,processed_pretrain_dataset],axis=1)\n",
    "# print(f\"size of pretrain start: {pretrain_start.shape}\")\n",
    "\n",
    "processed_train_dataset = pd.DataFrame(pca_train_data,columns=feature_cols)\n",
    "# print(f\"pretrain dataset: {processed_train_dataset}\")\n",
    "processed_train_dataset = pd.concat([train_start,processed_train_dataset],axis=1)\n",
    "# print(f\"size of train start: {train_start.shape}\")\n",
    "\n",
    "processed_test_dataset = pd.DataFrame(pca_test_data,columns=feature_cols)\n",
    "# print(f\"pretrain dataset: {processed_test_dataset}\")\n",
    "processed_test_dataset = pd.concat([test_start,processed_test_dataset],axis=1)\n",
    "# print(f\"size of test start: {test_start.shape}\")\n",
    "\n",
    "print(f\"pretrain dataset concatenated: {processed_pretrain_dataset}\")\n",
    "print(f\"train dataset concatenated: {processed_train_dataset}\")\n",
    "print(f\"test dataset concatenated: {processed_test_dataset}\")\n",
    "\n",
    "processed_pretrain_dataset.to_csv('./processed_pretrain_dataset.csv')\n",
    "processed_train_dataset.to_csv('./processed_train_dataset.csv')\n",
    "processed_test_dataset.to_csv('./processed_test_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>NOTE If we want we can extract more features from the chemistry database to enhance input data</h4>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
