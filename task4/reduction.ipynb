{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing and PCA data reduction of input data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import sklearn.decomposition as pca\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load in datasets and perform pandas cleaning up before scaling</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_data = pd.read_csv('./pretrain_features.csv.zip')\n",
    "pretrain_start = pd.DataFrame(pretrain_data,columns=['Id','smiles'])\n",
    "pretrain_trimmed = pretrain_data.drop(['Id','smiles'], axis=1)\n",
    "\n",
    "train_data = pd.read_csv('./train_features.csv.zip')\n",
    "train_start = pd.DataFrame(train_data,columns=['Id','smiles'])\n",
    "train_trimmed = train_data.drop(['Id','smiles'], axis=1)\n",
    "\n",
    "test_data = pd.read_csv('./test_features.csv.zip')\n",
    "test_start = pd.DataFrame(test_data,columns=['Id','smiles'])\n",
    "test_trimmed = test_data.drop(['Id','smiles'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Preprocess data to achieve mean = 0 and var = 1 NOTE scaler trained on pretrain features but applied to all three</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(pretrain_trimmed)\n",
    "pretrain_data_processed = scaler.transform(pretrain_trimmed)\n",
    "train_data_processed = scaler.transform(train_trimmed)\n",
    "test_data_processed = scaler.transform(test_trimmed)\n",
    "# print(f\"mean of scaler: {data_processed.mean(axis=0)}\")\n",
    "# print(f\"size of scaler: {data_processed.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Apply dimensionality reduction to break the 1000 features down into principle components NOTE not sure if same pca is good procedure but I think the more similar the NN input the easier to apply transfer learning</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_handler = pca.PCA(n_components=10)  # TODO see with how many features per molecule we perform best \n",
    "pca_handler.fit(pretrain_data_processed)\n",
    "pca_pretrain_data = pca_handler.transform(pretrain_data_processed)\n",
    "pca_train_data = pca_handler.transform(train_data_processed)\n",
    "pca_test_data = pca_handler.transform(test_data_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Save the reduced datasets</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain dataset concatenated:           Id                                             smiles  feature_1  \\\n",
      "0          0  c1occ2c1c1ccc3cscc3c1c1ncc3cc(ccc3c21)-c1cccc2...  -4.626551   \n",
      "1          1  C1C=c2c(cc3ncc4c5[SiH2]C=Cc5oc4c3c2=C1)-c1scc2...   0.954428   \n",
      "2          2  C1C=c2c3cccnc3c3c4c[nH]cc4c4cc(cnc4c3c2=C1)-c1...  -4.419375   \n",
      "3          3  [SiH2]1C=Cc2c1csc2-c1cnc2c(c1)c1ccccc1c1cc3ccc...   1.282784   \n",
      "4          4        c1occ2c1c(cc1[se]c3ccncc3c21)-c1cccc2nsnc12  -3.170778   \n",
      "...      ...                                                ...        ...   \n",
      "49995  49995      C1cc2c3-c4[nH]ccc4-ncc3c3c4c[SiH2]cc4ccc3c2c1  -4.541318   \n",
      "49996  49996  [SiH2]1C=c2c3C=C([SiH2]c3c3c4cscc4c4C=C[SiH2]c...   0.619174   \n",
      "49997  49997              C1C=Cc2csc(C3=Cc4ccc5cc[se]c5c4C3)c12   5.223223   \n",
      "49998  49998  [SiH2]1C=c2c3cc([nH]c3c3c4ccccc4c4ccc5nsnc5c4c...  -3.073758   \n",
      "49999  49999  C1C=c2c3ccccc3c3c(c4ccc(cc4c4=C[SiH2]C=c34)-c3...  -3.611722   \n",
      "\n",
      "       feature_2  feature_3  feature_4  feature_5  feature_6  feature_7  \\\n",
      "0       1.878084  -2.120061  -2.837257  -0.558068  -1.163544  -1.802773   \n",
      "1      -3.375941   1.212747  -0.113535  -0.035171   4.587743   2.160261   \n",
      "2       1.609313   1.210385   1.303962   0.481177   0.933787  -1.796416   \n",
      "3       1.646818   0.844859  -3.627194   3.941808   1.735151  -1.016282   \n",
      "4       5.724367   0.072722  -1.600050   1.423811  -1.871022  -0.086497   \n",
      "...          ...        ...        ...        ...        ...        ...   \n",
      "49995  -3.585986   3.799124   5.160413  -0.478431   3.372195   0.526206   \n",
      "49996  -6.022769   2.736024  -1.661905   0.245281   2.202003  -2.369585   \n",
      "49997  -3.789001  -0.486584  -0.488294   2.546146  -0.174543   0.781199   \n",
      "49998   3.270000   3.283472  -0.047738   0.849815   4.253687   0.854696   \n",
      "49999  -3.145138   1.644837   5.085769  -3.150857   2.661965  -2.178504   \n",
      "\n",
      "       feature_8  feature_9  feature_10  \n",
      "0      -1.978816  -3.417407    2.080516  \n",
      "1       1.831947   0.376081   -1.268831  \n",
      "2      -3.207175  -3.555978   -4.706989  \n",
      "3       0.315312  -5.175358   -4.075374  \n",
      "4       0.129473   2.449185    1.418265  \n",
      "...          ...        ...         ...  \n",
      "49995   0.673620  -0.406788   -2.610066  \n",
      "49996   8.227363   1.512181   -0.655922  \n",
      "49997  -6.269472   2.724098   -1.266168  \n",
      "49998   3.690895   0.544787   -0.048257  \n",
      "49999   2.429967  -0.511519   -2.552543  \n",
      "\n",
      "[50000 rows x 12 columns]\n",
      "train dataset concatenated:        Id                                             smiles  feature_1  \\\n",
      "0   50000    C1C=c2c3ccoc3c3c4ccccc4c(cc3c2=C1)-c1scc2ccsc12  -0.397598   \n",
      "1   50001  c1cc([se]c1-c1sc(-c2cccc3nsnc23)c2nccnc12)-c1c...   2.204763   \n",
      "2   50002  [SiH2]1C=CC=C1c1cc2cnc3c(sc4ccc5c[nH]cc5c34)c2...  -1.320269   \n",
      "3   50003  C1C=c2ccc3c4cocc4c4c([se]c5cc(-c6cccs6)c6nsnc6...  -4.166385   \n",
      "4   50004  C1c(ccc1-c1sc(-c2nccc3nsnc23)c2ccoc12)-c1scc2c...   5.404258   \n",
      "..    ...                                                ...        ...   \n",
      "95  50095  C1C=c2c(cc3oc4c5CC=Cc5c5c[nH]cc5c4c3c2=C1)-c1c...  -3.503814   \n",
      "96  50096  C1C(=Cc2c1c1c(c3nsnc23)c2[nH]ccc2c2=C[SiH2]C=c...   1.692158   \n",
      "97  50097   c1scc2c1ccc1[nH]c3c4oc(cc4c4nsnc4c3c21)-c1ccccn1  -4.319816   \n",
      "98  50098  [SiH2]1c2cc[nH]c2-c2oc3cc(-c4nccc5nsnc45)c4nsn...   0.115023   \n",
      "99  50099  [SiH2]1cc2c(ccc(-c3cccc4nsnc34)c2c1)-c1ccc(nc1...  -1.165758   \n",
      "\n",
      "    feature_2  feature_3  feature_4  feature_5  feature_6  feature_7  \\\n",
      "0   -2.280204  -3.319130   2.824486  -1.803013  -1.117659   5.849454   \n",
      "1    5.467211   3.046068   1.293794   0.522939   0.180948   0.187164   \n",
      "2   -0.796380   0.902887  -3.525389  -0.178907  -1.121399  -4.655140   \n",
      "3    1.163720   1.748608   3.444590   0.557858  -1.747330   1.274682   \n",
      "4    4.931231   0.606583  -0.589352  -5.310139   1.545472   3.841824   \n",
      "..        ...        ...        ...        ...        ...        ...   \n",
      "95  -5.183381   1.370730  -1.367886  -3.582975  -0.966000   0.438599   \n",
      "96  -1.774666   1.966161   0.881580  -1.039153   5.481636   1.999651   \n",
      "97   2.895884  -1.326037  -2.591179  -1.805191   2.660510   0.907621   \n",
      "98   5.042821   3.600542  -2.563434  -1.605282   2.461494   1.946865   \n",
      "99   5.359956   4.400782   2.095066  -3.176034  -3.167632   3.127081   \n",
      "\n",
      "    feature_8  feature_9  feature_10  \n",
      "0   -0.461229  -2.416960    2.684854  \n",
      "1    0.507263   2.594033    1.776192  \n",
      "2    4.698942  -0.572987    3.165899  \n",
      "3   -2.069890   1.411081    5.488811  \n",
      "4    0.551558   5.073635    0.691773  \n",
      "..        ...        ...         ...  \n",
      "95  -3.593694   1.657248    0.022801  \n",
      "96   0.870915   3.080537   -0.120100  \n",
      "97  -0.285944   0.322513   -2.054856  \n",
      "98   1.543471   4.432817    0.641935  \n",
      "99   3.562535   0.318875   -3.718823  \n",
      "\n",
      "[100 rows x 12 columns]\n",
      "test dataset concatenated:          Id                                             smiles  feature_1  \\\n",
      "0     50100  c1cc2c(scc2[nH]1)-c1ccc(cn1)-c1sc(-c2scc3occc2...   7.504071   \n",
      "1     50101   [SiH2]1C=Cc2ncc3c4oc(cc4c4cocc4c3c12)-c1ccc[se]1  -0.663102   \n",
      "2     50102  C1C=c2c3c(oc4cc(C5=CC=CC5)c5nsnc5c34)c3ccccc3c...  -2.311254   \n",
      "3     50103       c1c[se]c(c1)-c1ccc2c(c1)oc1c3[nH]ccc3[se]c21   0.088774   \n",
      "4     50104            c1scc2c1ccc1sc3c([se]c4ccc5nsnc5c34)c21  -2.436914   \n",
      "...     ...                                                ...        ...   \n",
      "9995  60095     C1C=Cc2ccc3c4[nH]c(cc4ncc3c12)-c1scc2cc[nH]c12   2.970549   \n",
      "9996  60096  c1occ2c1ccc1ccc3c([se]c4cc(-c5cccnc5)c5nsnc5c3...  -3.249269   \n",
      "9997  60097      c1ncc(s1)-c1ccc(cn1)-c1ccc(-c2cccnc2)c2nsnc12   1.130763   \n",
      "9998  60098    C1c(ccc1-c1ccccc1)-c1sc(-c2ccc[nH]2)c2[nH]ccc12   4.956512   \n",
      "9999  60099  C1cc2cccc(-c3cnc(s3)-c3sc(-c4cccc5nsnc45)c4cc[...   3.116130   \n",
      "\n",
      "      feature_2  feature_3  feature_4  feature_5  feature_6  feature_7  \\\n",
      "0      1.761125  -5.508572   2.805544  -6.062393   0.392480   5.294520   \n",
      "1     -1.614667  -1.036107  -3.529405   3.927201  -4.023248  -1.134418   \n",
      "2     -0.539065   4.738670  -0.328676  -3.352004  -1.962852   1.888026   \n",
      "3     -0.887686  -3.613737   2.030211   1.578984  -2.071438  -1.700809   \n",
      "4      3.396049  -0.652070  -0.012602   3.475279   0.838059   0.971371   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "9995  -2.183869  -1.347710  -0.175668  -1.515696   6.132594  -0.466184   \n",
      "9996   4.789160  -1.478791  -1.224428   1.861386  -3.104861  -0.744275   \n",
      "9997   6.766577   2.350938   1.091324  -0.414942  -0.853239  -2.483782   \n",
      "9998  -0.873294  -1.108830   2.107149  -7.293171   3.195004  -5.413309   \n",
      "9999   2.237671   8.839944   3.920125  -0.277532  -0.449834  -0.787983   \n",
      "\n",
      "      feature_8  feature_9  feature_10  \n",
      "0      1.902413  -1.653629   -5.484898  \n",
      "1      1.993863  -0.526366   -1.739025  \n",
      "2     -5.425506   3.824573    1.650483  \n",
      "3     -0.211431   3.319723   -2.065999  \n",
      "4     -1.092872   1.507497    5.969668  \n",
      "...         ...        ...         ...  \n",
      "9995  -2.140774   0.051449   -2.205710  \n",
      "9996  -0.591278   0.290655    0.381802  \n",
      "9997  -1.418916  -3.622555   -5.008389  \n",
      "9998   1.634390   2.588928   -1.802374  \n",
      "9999   2.124041  -1.964690    2.443316  \n",
      "\n",
      "[10000 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "feature_cols = ['feature_' + str(i+1) for i in range(pca_train_data.shape[1])] #create label names for the new pca labels\n",
    "#create datasets\n",
    "processed_pretrain_dataset = pd.DataFrame(pca_pretrain_data,columns=feature_cols)\n",
    "# print(f\"pretrain dataset: {processed_pretrain_dataset}\")\n",
    "processed_pretrain_dataset = pd.concat([pretrain_start,processed_pretrain_dataset],axis=1)\n",
    "# print(f\"size of pretrain start: {pretrain_start.shape}\")\n",
    "\n",
    "processed_train_dataset = pd.DataFrame(pca_train_data,columns=feature_cols)\n",
    "# print(f\"pretrain dataset: {processed_train_dataset}\")\n",
    "processed_train_dataset = pd.concat([train_start,processed_train_dataset],axis=1)\n",
    "# print(f\"size of train start: {train_start.shape}\")\n",
    "\n",
    "processed_test_dataset = pd.DataFrame(pca_test_data,columns=feature_cols)\n",
    "# print(f\"pretrain dataset: {processed_test_dataset}\")\n",
    "processed_test_dataset = pd.concat([test_start,processed_test_dataset],axis=1)\n",
    "# print(f\"size of test start: {test_start.shape}\")\n",
    "\n",
    "print(f\"pretrain dataset concatenated: {processed_pretrain_dataset}\")\n",
    "print(f\"train dataset concatenated: {processed_train_dataset}\")\n",
    "print(f\"test dataset concatenated: {processed_test_dataset}\")\n",
    "\n",
    "processed_pretrain_dataset.to_csv('./processed_pretrain_dataset.csv',index=False)\n",
    "processed_train_dataset.to_csv('./processed_train_dataset.csv',index=False)\n",
    "processed_test_dataset.to_csv('./processed_test_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>NOTE If we want we can extract more features from the chemistry database to enhance input data</h4>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
