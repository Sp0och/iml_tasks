{"cells":[{"cell_type":"markdown","metadata":{"id":"x3QGzKXpp2cX"},"source":["# Idea\n","\n","Use keras resenet encoder, then custom decoder. Group together to form siamese triplets, with a stack of the three decoded features. Then minimize the distance between the three."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9775,"status":"ok","timestamp":1651764501183,"user":{"displayName":"Jonathan","userId":"11399724922527259561"},"user_tz":-120},"id":"EEiW7OabpZCW","outputId":"6e046661-28c6-4ae4-877f-73fdf1ff0a75"},"outputs":[],"source":["!pip install --upgrade tensorflow-datasets\n","!pip install -U tensorboard_plugin_profile\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import tensorboard_plugin_profile\n","\n","# check if we have available GPU\n","if tf.config.list_physical_devices('GPU'):\n","  print(\"HAS GPU\")"]},{"cell_type":"markdown","metadata":{"id":"SH9aHOhTHt-y"},"source":["Mount the google drive with uploaded input data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4237,"status":"ok","timestamp":1651764332122,"user":{"displayName":"Jonathan","userId":"11399724922527259561"},"user_tz":-120},"id":"7grPQXm1Gvnl","outputId":"e12d1889-52ce-4cc6-9f85-7afeb064bb5a"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","path_to_folder = \"/content/drive/MyDrive/IML_Task3/\"\n","path_to_food = \"/content/drive/MyDrive/IML_Task3/food/\""]},{"cell_type":"markdown","metadata":{"id":"LN8pZHw3jHoJ"},"source":["On a local runtime, run the following instead:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5hbCWuXDjGCN"},"outputs":[],"source":["path_to_folder = \"./\"\n","path_to_food = \"./food/\""]},{"cell_type":"markdown","metadata":{"id":"cPGG01vqsVWz"},"source":["Loading from google drive\n","Upload the txt files via the upload dialog to the left, the images take too long thats why we do the below steps:\n","\n","https://www.youtube.com/watch?v=Mq8-WdcnzVo <-- source\n","\n","--> Since accessing rom google drive is quite slow, instead download the zip folder and unzip it, \n","- add the zip folder to your google drive, \n","- left click on it --> \"Link abrufen\" --> freigeben f√ºr alle, link kopieren: \n","- then just copy the id of the link, which is between the d/ and /view, \n","\n","for example: ...gle.com/file/d/**1RNc879PiOQaVDLhMmygpGA4rfscFrmWa**/view?usp=sha...\n","\n","insert this id for the drive_id below:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47319,"status":"ok","timestamp":1651764382501,"user":{"displayName":"Jonathan","userId":"11399724922527259561"},"user_tz":-120},"id":"GGMbFRmztHU6","outputId":"eeaf645e-23dc-498b-abe4-bd39eb81914a"},"outputs":[],"source":["! gdown --id 1LsF0_SND4REqTZWpxCZBAD9SInvFD3A5 # food.zip \n","! unzip /content/food.zip -d /content/\n","path_to_food = \"/content/food/\""]},{"cell_type":"markdown","metadata":{"id":"JCqYWswAp0hB"},"source":["# Data Loading (JONNY)\n","First create a dataset containing only the number triplets"]},{"cell_type":"code","execution_count":65,"metadata":{"executionInfo":{"elapsed":305,"status":"ok","timestamp":1651764407378,"user":{"displayName":"Jonathan","userId":"11399724922527259561"},"user_tz":-120},"id":"GhfvDjv9S-xE"},"outputs":[],"source":["# resnet input dimensions 224 x 224 x 3\n","# efficient net input size 256\n","IMG_HEIGHT = 256\n","IMG_WIDTH = 256\n","TRAIN_DATASET_SIZE = 0  # gets overwritten in load_dataset\n","VAL_DATASET_SIZE = 0    # gets overwritten in load_dataset\n","TEST_DATASET_SIZE = 0   # gets overwritten in load_dataset\n","BATCH_SIZE = 64\n","TEST_BATCH_SIZE = 128\n","AUTOTUNE = tf.data.AUTOTUNE\n","TRAIN_VALID_SPLIT = 0.8"]},{"cell_type":"code","execution_count":66,"metadata":{"executionInfo":{"elapsed":317,"status":"ok","timestamp":1651764409528,"user":{"displayName":"Jonathan","userId":"11399724922527259561"},"user_tz":-120},"id":"qibEbZsB1XgC"},"outputs":[],"source":["# TODO move these paths down\n","path_to_train = path_to_folder + \"train_triplets.txt\"\n","path_to_test = path_to_folder + \"test_triplets.txt\"\n","\n","def load_image(img):\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.cast(img, tf.float32)\n","    img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n","    return img\n","\n","def get_image_triplet(num_triplet, is_train):\n","  print(path_to_food)\n","  imA = load_image(tf.io.read_file(path_to_food + num_triplet[0] + '.jpg'))\n","  imB = load_image(tf.io.read_file(path_to_food + num_triplet[1] + '.jpg'))\n","  imC = load_image(tf.io.read_file(path_to_food + num_triplet[2] + '.jpg'))\n","  # in training append label = 1\n","  if is_train:\n","    return tf.stack([imA, imB, imC], axis=0), 1\n","  else:\n","    return tf.stack([imA, imB, imC], axis=0)\n","\n","def load_dataset(path, is_train):\n","  data_array = np.loadtxt(path, dtype=str)\n","  # data_array = data_array[0:1000] <--- can be used to test with small amount of data\n","  dataset = tf.data.Dataset.from_tensor_slices(data_array) # num1 num2 num3 as string\n","\n","  if is_train:\n","    global TRAIN_DATASET_SIZE \n","    global VAL_DATASET_SIZE\n","    TRAIN_DATASET_SIZE = (TRAIN_VALID_SPLIT * dataset.cardinality().numpy())\n","    VAL_DATASET_SIZE = dataset.cardinality().numpy() - TRAIN_DATASET_SIZE\n","\n","    train_dataset = dataset.take(TRAIN_DATASET_SIZE)\\\n","                            .shuffle(BATCH_SIZE, reshuffle_each_iteration=True)\\\n","                            .repeat()\n","    \n","    val_dataset = dataset.skip(TRAIN_DATASET_SIZE).repeat()\n","    \n","    train_dataset = train_dataset.map(lambda num_triplet: get_image_triplet(num_triplet, is_train),\n","      num_parallel_calls=AUTOTUNE)\n","    val_dataset = val_dataset.map(lambda num_triplet: get_image_triplet(num_triplet, is_train),\n","      num_parallel_calls=AUTOTUNE)\n","    \n","    return train_dataset, val_dataset\n","\n","  else:\n","    global TEST_DATASET_SIZE\n","    TEST_DATASET_SIZE = dataset.cardinality().numpy()\n","    # apply transformation to images\n","    dataset = dataset.map(lambda num_triplet: get_image_triplet(num_triplet, is_train),\n","        num_parallel_calls=AUTOTUNE)\n","    return dataset"]},{"cell_type":"markdown","metadata":{"id":"QGPIYdVIBxwh"},"source":["## Loading the datasets\n","Load the train, validation and test datasets, with a split of 80%/20% for the validation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["cf9a98e2445849e195d1b870fc2648f0","82d584992d68420ab75b23a2dadd2a80","43981a9a253b44068d7ff05cf794574b","23fb5beadd484924b960de5fc004dab4","e829e0e1012c4ceaa7067832c85022e4","82d8086b7c4a4220bb6d759f14ccd8e6","dca867a427c142e9a98f183b2dfb3b17","099cbd8c76f04772afc6883983247e0d","c66270ee3a0d43c9bf8ce75ec46f9d7b","5696ad4cac754a8fba5bb30f62843b34","e3d9bc92e44b4142b4fb7fae8d801e69"]},"executionInfo":{"elapsed":24340,"status":"ok","timestamp":1651764570464,"user":{"displayName":"Jonathan","userId":"11399724922527259561"},"user_tz":-120},"id":"UVCf6u6UaQvt","outputId":"d732f542-4007-4bbe-91bb-f8a4715b6f15"},"outputs":[],"source":["train_dataset, val_dataset = load_dataset(path_to_train, is_train=True)\n","\n","train_dataset = train_dataset.batch(BATCH_SIZE)\n","val_dataset = val_dataset.batch(BATCH_SIZE)\n","\n","test_dataset = load_dataset(path_to_test, is_train=False).batch(TEST_BATCH_SIZE)\n","\n","# benchmark how long a loading a batch takes\n","tfds.benchmark(train_dataset, batch_size=BATCH_SIZE, num_iter=10)\n","\n","VAL_DATASET_SIZE = np.ceil(TRAIN_DATASET_SIZE*(1.0-TRAIN_VALID_SPLIT))\n","TRAIN_DATASET_SIZE -= VAL_DATASET_SIZE\n","\n","for batch in train_dataset.take(1):\n","  plt.figure(figsize=(10, 10))\n","  images = batch[0][0]\n","  for i in range(3):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(images[i] / 255)\n","    plt.axis('off')"]},{"cell_type":"markdown","metadata":{"id":"DJQRfUvWBdYn"},"source":["Use buffered prefetching to remove I/O bottleneck in image loading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":328,"status":"ok","timestamp":1651764573478,"user":{"displayName":"Jonathan","userId":"11399724922527259561"},"user_tz":-120},"id":"9F5jrrjzBjh8","outputId":"9cfb978e-fd90-4c62-8d8d-789365117f0a"},"outputs":[],"source":["print(f\"type train dataset: {type(train_dataset)}\")\n","train_dataset = train_dataset.prefetch(buffer_size=4)\n","val_dataset = val_dataset.prefetch(buffer_size=4)\n","test_dataset = test_dataset.prefetch(buffer_size=4)"]},{"cell_type":"markdown","metadata":{"id":"pZETYqhjbcW-"},"source":["To avoid overfitting and enhance the dataset we introduce data augmentation during training by randomly flipping and rotating the images. This layer is automatically only active during training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGRXjIjMbb0q"},"outputs":[],"source":["data_augm_layer = tf.keras.Sequential([\n","  tf.keras.layers.RandomFlip('horizontal'),\n","  tf.keras.layers.RandomRotation(0.2),\n","])"]},{"cell_type":"markdown","metadata":{"id":"qnX1sr20cam-"},"source":["We also need to preprocess the images to bring them into the correct format, although **for Efficient net, this is not required** as the preprocessing layer is part of the model already. For Resnet we would use:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLZ50UcWcee5"},"outputs":[],"source":["# preproc_layer = tf.keras.applications.resnet.preprocess_input "]},{"cell_type":"markdown","metadata":{"id":"zJcWm3J2bITg"},"source":["# TODO Data Selection\n","\n","To speed up training and convergence, we select a part of the data that is most relevant for training. Triplets where the reference image is very similar to the postive match and very different to the negative match don't provide meaningful training. Imagine the corner case of the anchor image being white, the correct match also being white and the incorrect match being black, this triplet will be correctly matched with very little influence of the network. As detailed in this paper: https://arxiv.org/pdf/1503.03832.pdf for training it is ideal to select those triplets that are hard to match, i.e. just looking at the image the similar ones looke very different and the different ones look similar.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KVwZ71puVwjN"},"source":["# Loss\n","We implement a triplet loss as shown in this example for image similarity estimation https://keras.io/examples/vision/siamese_network/.\n","\n","First we find the squared difference between the extracted features to get an estimate of similarity. We want the difference between the actual image the the similar image to be small -> i.e. the loss should decrease if this difference decreases. We also want to maximumize the difference between the reference and the different image, i.e. the loss should decrease if the difference increases. We can achieve the latter by adding a \"-\" sign to the latter difference.\n","\n","The loss is then\n","‚Äñf(A) - f(P)‚Äñ¬≤ - ‚Äñf(A) - f(N)‚Äñ¬≤\n","\n","Where f(.) are the output features of the network.\n","\n","Additionally we can enforce a margin between positive and negative pairs by adding it to the cost function as detailed in https://arxiv.org/pdf/1503.03832.pdf. This can be imagined like moving the softplus function upwards.\n","\n","Different than the link above, we then finally constrain the loss to be always positive with a softplus function to guarantee that the loss always remains differentiable."]},{"cell_type":"code","execution_count":72,"metadata":{"executionInfo":{"elapsed":235,"status":"ok","timestamp":1651764580869,"user":{"displayName":"Jonathan","userId":"11399724922527259561"},"user_tz":-120},"id":"_t3wJG21ZkJz"},"outputs":[],"source":["SIMILARITY_MARGIN = 0.1\n","\n","def calc_difference(out_features):\n","    '''Compute the difference between the considered image and the alledged \n","    similar and different one respectively. Returns two positive numbers'''\n","    image = out_features[...,0]\n","    similar = out_features[...,1] \n","    different = out_features[...,2]\n","    # sum of squared differences\n","    sim_diff = tf.reduce_sum(tf.square(image - similar),1)\n","    dif_diff = tf.reduce_sum(tf.square(image - different),1)\n","    return sim_diff, dif_diff\n","\n","def triplet_loss_function(y_true,out_features):\n","    '''loss function to minimize during back propagation\\n\n","       One has to define a loss function with y_true and y_pred as arguments'''\n","    sim_diff, dif_diff = calc_difference(out_features)\n","    return tf.reduce_mean(tf.math.softplus(sim_diff-dif_diff + SIMILARITY_MARGIN))"]},{"cell_type":"markdown","metadata":{"id":"jsgThFjEj8_u"},"source":["# Profiling\n","\n","To find bottlenecks, we add a profiler which writes to the log directory"]},{"cell_type":"code","execution_count":73,"metadata":{"executionInfo":{"elapsed":209,"status":"ok","timestamp":1651764582284,"user":{"displayName":"Jonathan","userId":"11399724922527259561"},"user_tz":-120},"id":"MV-_Hq2spHJE"},"outputs":[],"source":["log_dir = path_to_folder + \"log/\"\n","\n","# callback only active for batches 10 to 15\n","tb_callback = tf.keras.callbacks.TensorBoard(\n","    log_dir=log_dir, write_steps_per_second=True, profile_batch=(10, 15))\n"]},{"cell_type":"markdown","metadata":{"id":"B2gx4X0Fx5pB"},"source":["# Creating The Model (LASSE UND ADRIAN)\n","\n","The model should have a pretrained encoder, apply them to all three images and stich them together with custom layers. ADRIAN AND LASSE"]},{"cell_type":"code","execution_count":74,"metadata":{"executionInfo":{"elapsed":308,"status":"ok","timestamp":1651764584118,"user":{"displayName":"Jonathan","userId":"11399724922527259561"},"user_tz":-120},"id":"oVEZQnzlMN24"},"outputs":[],"source":["class model_manager:\n","    def __init__(self,img_height,img_width):\n","        self.IMG_HEIGHT = img_height\n","        self.IMG_WIDTH = img_width\n","        self.inputs = tf.keras.Input(shape=(3, self.IMG_HEIGHT, self.IMG_WIDTH, 3))\n","        #create data augmentation by randomly switching horizontally and randomly rotate by 0.1\n","        data_augmentation = tf.keras.Sequential(\n","            [\n","                tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n","                tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n","            ]\n","        )\n","        #EfficientNet does not need preprocessing and expects inputs in tensor form in the range 0-255\n","        encoder =  tf.keras.applications.EfficientNetB4(\n","            include_top=False, input_shape=(self.IMG_HEIGHT, self.IMG_WIDTH, 3), \n","            weights=\"imagenet\")\n","        encoder.trainable = False\n","        decoder = tf.keras.Sequential([\n","            tf.keras.layers.GlobalAveragePooling2D(),\n","            tf.keras.layers.Dense(32),\n","            tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n","        ])\n","        image, similar, different = self.inputs[:, 0, ...], self.inputs[:, 1, ...], self.inputs[:, 2, ...]\n","        image_features = decoder(encoder(data_augmentation(image)))\n","        similar_features = decoder(encoder(data_augmentation(similar)))\n","        different_features = decoder(encoder(data_augmentation(different)))\n","        out_features = tf.stack([image_features,similar_features,different_features],axis=-1)\n","        self.model = tf.keras.Model(inputs=self.inputs,outputs=out_features)\n","        print(\"Successfully built basic model!\")\n","\n","    def compile(self):\n","        print(\"Compilation Initiated...\")\n","        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),\n","                            loss=triplet_loss_function,\n","                            metrics=triplet_loss_function)\n","        print(\"NN compiled successfully!\")\n","\n","    def fit(self,_train_ds, _valid_ds,  _epochs=1 ,_verbose=1):\n","        #59515 being the length of the training samples\n","        print(f\"Train Batch size: {BATCH_SIZE}\")\n","        print(f\"Validation Batch size: {BATCH_SIZE}\")\n","        print(f\"Epochs: {_epochs}\")\n","        # ceil ensures we see all data at least once\n","        train_steps = np.ceil(TRAIN_DATASET_SIZE/BATCH_SIZE)\n","        print(f\"Train Steps: {train_steps}\")\n","        # hardcode val steps as we don't need to take all data\n","        val_steps = 5 #np.ceil(VAL_DATASET_SIZE/BATCH_SIZE)\n","        print(f\"Validation Steps: {val_steps}\")\n","        print(f\"Starting fitting procedure...\")\n","        self.model.fit(_train_ds,\n","                       batch_size=BATCH_SIZE, \n","                       epochs=_epochs,\n","                       validation_data=_valid_ds,\n","                       verbose=_verbose, \n","                       steps_per_epoch=train_steps, \n","                       validation_steps=val_steps, \n","                       callbacks=[tb_callback]) # this adds the profiler \n","        print(f\"Fitting procedure finished!\")\n","\n","    def add_predictor(self):\n","        sim_diff, dif_diff = calc_difference(self.model.output)\n","        prediction = tf.cast(tf.greater_equal(dif_diff,sim_diff),tf.int8)\n","        self.model = tf.keras.Model(inputs=self.model.input,outputs=prediction)"]},{"cell_type":"markdown","metadata":{"id":"w47j7ccILOAt"},"source":["# Main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5gahoB81LuPs"},"outputs":[],"source":["# check if we have available GPU\n","if not tf.config.list_physical_devices('GPU'):\n","  has_gpu = False\n","  print(\"TRAINING WITHOUT GPU MIGHT TAKE FOREVER\")\n","  # disable this logging, as it will otherwise spam the output\n","else:\n","  has_gpu = True\n","\n","with tf.device('/device:GPU:0') if has_gpu else tf.device('/device:CPU:0'):\n","  manager = model_manager(256,256)\n","  #manager.compile()\n","  #manager.fit(train_dataset, val_dataset)\n","  #check for saved model or create new \n","  if(os.path.isdir(path_to_folder + \"model_10\")):\n","    # as we load only for predicting, we can set compile to false and don't\n","    # need to pass the loss function\n","    manager.model = tf.keras.models.load_model(path_to_folder + \"model_10\", compile=False)\n","    print(\"Successfully loaded model!!!!\")\n","    manager.model.summary()\n","  else:\n","    manager.compile()\n","    manager.model.summary()\n","    manager.fit(train_dataset, val_dataset, _epochs=10)\n","    manager.model.save(path_to_folder + \"model_10\")\n","\n","  # sim_diff, dif_diff = manager.calc_difference()\n","  # greq = tf.greater_equal(dif_diff,sim_diff)\n","  # prediction = tf.cast(greq,tf.int8)\n","  # manager.model = tf.keras.Model(inputs=manager.inputs,outputs=prediction)\n","  manager.add_predictor()\n","  print(\"Start Prediction\")\n","  print(f\"Batch Size: {TEST_BATCH_SIZE}\")\n","  print(f\"Dataset Size: {TEST_DATASET_SIZE}\")\n","  # ceil ensures we see all data at least once\n","  test_steps = np.ceil(TEST_DATASET_SIZE/TEST_BATCH_SIZE)\n","  print(f\"Steps: {test_steps}\")\n","  # uncomment to test out with one batch:\n","  # predictions = manager.model.predict(test_dataset.take(1), verbose=1, steps=1)\n","  predictions = manager.model.predict(test_dataset, verbose=1, steps=test_steps)\n","  np.savetxt(path_to_folder + 'predictions.txt', predictions,fmt='%i')"]},{"cell_type":"markdown","metadata":{"id":"3XES0YDbm_WA"},"source":["# Analyse Runtime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IuZ2ysfYnBIS"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir=log_dir\n","\n","# if nothing shows up, uncomment the following instead open a browser and open http://localhost:8080 \n","# %tensorboard --logdir=log_dir --host=127.0.0.1 --port=8080"]},{"cell_type":"markdown","metadata":{"id":"D6zPamqgh3wx"},"source":["# TODO: Visualize Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8f_5swPCm8DJ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"W_ofivAfuue8"},"source":["# Testing Section\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"IMLresnet.ipynb","provenance":[{"file_id":"https://github.com/d2l-ai/d2l-en-colab/blob/master/chapter_convolutional-modern/resnet.ipynb","timestamp":1651156802911}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.11"},"widgets":{"application/vnd.jupyter.widget-state+json":{"099cbd8c76f04772afc6883983247e0d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"23fb5beadd484924b960de5fc004dab4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5696ad4cac754a8fba5bb30f62843b34","placeholder":"‚Äã","style":"IPY_MODEL_e3d9bc92e44b4142b4fb7fae8d801e69","value":" 12/? [00:09&lt;00:00,  1.28it/s]"}},"43981a9a253b44068d7ff05cf794574b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_099cbd8c76f04772afc6883983247e0d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c66270ee3a0d43c9bf8ce75ec46f9d7b","value":1}},"5696ad4cac754a8fba5bb30f62843b34":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82d584992d68420ab75b23a2dadd2a80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82d8086b7c4a4220bb6d759f14ccd8e6","placeholder":"‚Äã","style":"IPY_MODEL_dca867a427c142e9a98f183b2dfb3b17","value":""}},"82d8086b7c4a4220bb6d759f14ccd8e6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c66270ee3a0d43c9bf8ce75ec46f9d7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cf9a98e2445849e195d1b870fc2648f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_82d584992d68420ab75b23a2dadd2a80","IPY_MODEL_43981a9a253b44068d7ff05cf794574b","IPY_MODEL_23fb5beadd484924b960de5fc004dab4"],"layout":"IPY_MODEL_e829e0e1012c4ceaa7067832c85022e4"}},"dca867a427c142e9a98f183b2dfb3b17":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3d9bc92e44b4142b4fb7fae8d801e69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e829e0e1012c4ceaa7067832c85022e4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
